{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "This is just common setup so that we can load the right flyte configuration and the connect with the right endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to flyte.lyft.net False\n"
     ]
    }
   ],
   "source": [
    "from flytekit.configuration import set_flyte_config_file, platform\n",
    "#set_flyte_config_file(\"/Users/kumare/.ssh/notebook-staging.config\")\n",
    "set_flyte_config_file(\"../flyte-production.config\")\n",
    "\n",
    "print(\"Connected to {} {}\".format(platform.URL.get(), platform.AUTH.get()))\n",
    "\n",
    "def print_console_url(exc):\n",
    "    print(\"http://{}/console/projects/{}/domains/{}/executions/{}\".format(platform.URL.get(), exc.id.project, exc.id.domain, exc.id.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a XGBoost Training job\n",
    "We will create a job that will train an XGBoost model using the prebuilt algorithms @Sagemaker. Refer to [Sagemaker XGBoost docs here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html). To understand more about XGBoost refer [here](https://xgboost.readthedocs.io/en/latest/). To dive deeper into the Flytekit API refer to [docs](https://lyft.github.io/flyte/flytekit/flytekit.common.tasks.sagemaker.html?highlight=sagemaker#module-flytekit.common.tasks.sagemaker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flytekit.sdk.tasks import inputs\n",
    "from flytekit.sdk.types import Types\n",
    "from flytekit.sdk.workflow import workflow_class, Input, Output\n",
    "from flytekit.common.tasks.sagemaker import built_in_training_job_task\n",
    "from flytekit.models.sagemaker import training_job as training_job_models\n",
    "\n",
    "# Defining the values of some hyperparameters, which will be used by the TrainingJob \n",
    "# these hyper-parameters are commonly used by the XGboost algorithm. Here we bootstrap them with some default Values\n",
    "# Usually the default values are selected or \"tuned - refer to next section\"\n",
    "xgboost_hyperparameters = {\n",
    "    \"num_round\": \"100\",\n",
    "    \"base_score\": \"0.5\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"csv_weights\": \"0\",\n",
    "    \"dsplit\": \"row\",\n",
    "    \"grow_policy\": \"depthwise\",\n",
    "    \"lambda_bias\": \"0.0\",\n",
    "    \"max_bin\": \"256\",\n",
    "    \"normalize_type\": \"tree\",\n",
    "    \"objective\": \"reg:linear\",\n",
    "    \"one_drop\": \"0\",\n",
    "    \"process_type\": \"default\",\n",
    "    \"refresh_leaf\": \"1\",\n",
    "    \"sample_type\": \"uniform\",\n",
    "    \"scale_pos_weight\": \"1.0\",\n",
    "    \"silent\": \"0\",\n",
    "    \"skip_drop\": \"0.0\",\n",
    "    \"tree_method\": \"auto\",\n",
    "    \"tweedie_variance_power\": \"1.5\",\n",
    "    \"updater\": \"grow_colmaker,prune\",\n",
    "}\n",
    "\n",
    "# Here we define the actual algorithm (XGBOOST) and version of the algorithm to use\n",
    "alg_spec = training_job_models.AlgorithmSpecification(\n",
    "    input_mode=training_job_models.InputMode.FILE,\n",
    "    algorithm_name=training_job_models.AlgorithmName.XGBOOST,\n",
    "    algorithm_version=\"0.90\",\n",
    "    input_content_type=training_job_models.InputContentType.TEXT_CSV,\n",
    ")\n",
    "\n",
    "# Finally lets use Flytekit plugin called SdkBuiltinAlgorithmTrainingJobTask, to create a task that wraps the algorithm.\n",
    "# This task does not really have a user-defined function as the actual algorithm is pre-defined in Sagemaker.\n",
    "# But, this task still has the same set of properties like any other FlyteTask\n",
    "# - Caching\n",
    "# - Resource specification\n",
    "# - versioning etc\n",
    "xgboost_train_task = built_in_training_job_task.SdkBuiltinAlgorithmTrainingJobTask(\n",
    "    training_job_resource_config=training_job_models.TrainingJobResourceConfig(\n",
    "        instance_type=\"ml.m4.xlarge\",\n",
    "        instance_count=1,\n",
    "        volume_size_in_gb=25,\n",
    "    ),\n",
    "    algorithm_specification=alg_spec,\n",
    "    cache_version='blah9',\n",
    "    cacheable=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the training job\n",
    "You can use [Single task Execution](https://lyft.github.io/flyte/user/features/single_task_execution.html?highlight=single%20task%20execution) to trigger an execution for the defined training task. To trigger an execution, you need to provide\n",
    "1. Project (flyteexamples)\n",
    "1. Domain (development)\n",
    "1. inputs\n",
    "\n",
    "Pre-built algorithms have restricitve set of inputs. They always expect\n",
    "1. Training data set\n",
    "1. Validation data set\n",
    "1. Static set of hyper parameters as a dictionary\n",
    "\n",
    "In this case lets take the input generated for the single location training as shown in the previous example. To get the outputs, we will fetch the outputs from the previous execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://flyte.lyft.net/console/projects/flytesnacks/domains/development/executions/ctwv59t4g9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': metadata {\n",
       "   type {\n",
       "   }\n",
       " }\n",
       " uri: \"s3://lyft-modelbuilder/3g/ctwv59t4g9-fit-task-0/0e1a85186c9f6255216738e5a2810fb6\",\n",
       " 'accuracy': 0.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flytekit.common.workflow_execution import SdkWorkflowExecution\n",
    "exe_id = \"ctwv59t4g9\" # Replace this\n",
    "exe = SdkWorkflowExecution.fetch(project=\"flytesnacks\", domain=\"development\", name=exe_id)\n",
    "print_console_url(exe)\n",
    "exe.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have the execution now\n",
    "As we can see the outputs are available locally. But we dont want the outputs of the workflow, we want the intermediate generated data - the intermediate outputs. This was output of node execution. The node in this case is \"split\" the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = exe.get_node_executions()\n",
    "op = nodes[\"split\"].outputs\n",
    "train = op[\"train\"]\n",
    "val = op[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://flyte.lyft.net/console/projects/flytesnacks/domains/development/executions/o4zh55a04r\n"
     ]
    }
   ],
   "source": [
    "training_inputs={\n",
    "    \"train\": train.uri,\n",
    "    \"validation\": val.uri,\n",
    "    \"static_hyperparameters\": xgboost_hyperparameters,\n",
    "}\n",
    "\n",
    "# Invoking the SdkBuiltinAlgorithmTrainingJobTask\n",
    "training_exc = xgboost_train_task.register_and_launch(\"flytesnacks\", \"development\", inputs=training_inputs)\n",
    "print_console_url(training_exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the hyper-parameters\n",
    "Amazon Sagemaker offers automatic hyper parameter blackbox optimization using [HPO Service](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html). This technique is highly effective to figure out the right set of hyper parameters to use that improve the overall accuracy of the model (or minimize the error)\n",
    "\n",
    "Flyte makes it extremely effective to optimize a model using Amazon Sagemaker HPO. In this example we will look how this can be done for the prebuilt algorithm training done in the previous section\n",
    "\n",
    "### Define an HPO Task, that wraps the training task\n",
    "So to start with hyper parameter optimization, once a training task is created, wrap it in **SdkSimpleHyperparameterTuningJobTask** attribte **training_job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flytekit.models.sagemaker import hpo_job as hpo_job_models\n",
    "from flytekit.common.tasks.sagemaker import hpo_job_task\n",
    "from flytekit.models.sagemaker.hpo_job import HyperparameterTuningJobConfig, \\\n",
    "HyperparameterTuningObjectiveType, HyperparameterTuningStrategy, \\\n",
    "TrainingJobEarlyStoppingType, HyperparameterTuningObjective\n",
    "from flytekit.models.sagemaker.parameter_ranges import HyperparameterScalingType, ParameterRanges, CategoricalParameterRange, ContinuousParameterRange, IntegerParameterRange\n",
    "\n",
    "xgboost_hpo_task = hpo_job_task.SdkSimpleHyperparameterTuningJobTask(\n",
    "    training_job=xgboost_train_task,\n",
    "    max_number_of_training_jobs=10,\n",
    "    max_parallel_training_jobs=5,\n",
    "    cache_version='blah9',\n",
    "    retries=2,\n",
    "    cacheable=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the HPO Job\n",
    "Just like Training Job, HPO job can be launched directly from the notebook\n",
    "The inputs for an HPO job that wraps a training job, are the combination of all inputs for the training job - i.e,\n",
    " 1. \"train\" dataset, \"validation\" dataset and \"static hyper parameters\" for Training job\n",
    " 1. HyperparameterTuningJobConfig, which consists of ParameterRanges, for the parameters that should be tuned,\n",
    " 1. tuning strategy - Bayesian OR Random (or others as described in Sagemaker)\n",
    " 1. Stopping condition and\n",
    " 1. Objective metric name and type (whether to minimize etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://flyte.lyft.net/console/projects/flytesnacks/domains/development/executions/mhgakfah9h\n"
     ]
    }
   ],
   "source": [
    "# When launching the TrainingJob and HPOJob, we need to define the inputs.\n",
    "# Inputs are those directly related to algorithm outputs. We use the inputs\n",
    "# and the version information to decide cache hit/miss\n",
    "\n",
    "hpo_inputs={\n",
    "    \"train\": train.uri,\n",
    "    \"validation\": val.uri,\n",
    "    \"static_hyperparameters\": xgboost_hyperparameters,\n",
    "    \"hyperparameter_tuning_job_config\": HyperparameterTuningJobConfig(\n",
    "        \n",
    "        #############################################\n",
    "        # Define the tunable hyperparameters and the \n",
    "        # range/set of possible values of each hp\n",
    "        #############################################\n",
    "        hyperparameter_ranges=ParameterRanges(\n",
    "            parameter_range_map={\n",
    "                \"num_round\": IntegerParameterRange(min_value=3, max_value=10, \n",
    "                                                   scaling_type=HyperparameterScalingType.LINEAR),\n",
    "                \"max_depth\": IntegerParameterRange(min_value=5, max_value=7, \n",
    "                                                   scaling_type=HyperparameterScalingType.LINEAR),\n",
    "                \"gamma\": ContinuousParameterRange(min_value=0.0, max_value=0.3,\n",
    "                                                  scaling_type=HyperparameterScalingType.LINEAR),\n",
    "            }\n",
    "        ),\n",
    "        tuning_strategy=HyperparameterTuningStrategy.BAYESIAN,\n",
    "        tuning_objective=HyperparameterTuningObjective(\n",
    "            objective_type=HyperparameterTuningObjectiveType.MINIMIZE,\n",
    "            metric_name=\"validation:error\",\n",
    "        ),\n",
    "        training_job_early_stopping_type=TrainingJobEarlyStoppingType.AUTO\n",
    "    ).to_flyte_idl(),\n",
    "}\n",
    "\n",
    "## Register and launch the task standalone!\n",
    "hpo_exc = xgboost_hpo_task.register_and_launch(\"flytesnacks\", \"development\", inputs=hpo_inputs)\n",
    "print_console_url(hpo_exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can actually create a pipeline dynamically in the notebook\n",
    "Let us use the task that would generate the data\n",
    "\"demo.house_price_predictor.generate_and_split_data\"\n",
    "And then create a workflow that chains this task with the sagemaker training task (Of course you can do this for the HPO Task as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flyte python-task: inputs {\n",
       "  variables {\n",
       "    key: \"loc\"\n",
       "    value {\n",
       "      type {\n",
       "        simple: STRING\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  variables {\n",
       "    key: \"number_of_houses\"\n",
       "    value {\n",
       "      type {\n",
       "        simple: INTEGER\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  variables {\n",
       "    key: \"seed\"\n",
       "    value {\n",
       "      type {\n",
       "        simple: INTEGER\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "outputs {\n",
       "  variables {\n",
       "    key: \"test\"\n",
       "    value {\n",
       "      type {\n",
       "        blob {\n",
       "          format: \"csv\"\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  variables {\n",
       "    key: \"train\"\n",
       "    value {\n",
       "      type {\n",
       "        blob {\n",
       "          format: \"csv\"\n",
       "          dimensionality: MULTIPART\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  variables {\n",
       "    key: \"val\"\n",
       "    value {\n",
       "      type {\n",
       "        blob {\n",
       "          format: \"csv\"\n",
       "          dimensionality: MULTIPART\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flytekit.common.tasks.task import SdkTask\n",
    "generate_data = SdkTask.fetch(\"flytesnacks\", \"development\", \"demo.house_price_predictor.generate_and_split_data\", version=\"85dc5898ae0d9b375e194aa1246b548ab95242d7\")\n",
    "generate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flytekit.sdk.types import Types\n",
    "from flytekit.sdk.workflow import workflow_class, Input, Output, workflow\n",
    "\n",
    "@workflow_class\n",
    "class WorkflowFromSharedTasks():\n",
    "    loc = Input(Types.String, help=\"Location for where to train the model.\")\n",
    "    seed = Input(Types.Integer, default=7, help=\"Seed to use for splitting.\")\n",
    "    num_houses = Input(Types.Integer, default=1000, help=\"Number of houses to generate data for\")\n",
    "    generate_data_task = generate_data(loc=loc, number_of_houses=num_houses, seed=seed)\n",
    "    fit_task = xgboost_train_task(train=generate_data_task.outputs.train, validation=generate_data_task.outputs.val, static_hyperparameters=xgboost_hyperparameters)\n",
    "\n",
    "    # Outputs: joblib seralized model and accuracy of the model\n",
    "    model = Output(fit_task.outputs.model, sdk_type=Types.Blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_lp = WorkflowFromSharedTasks.create_launch_plan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lp:flytesnacks:development:SagemakerShared:1'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WorkflowFromSharedTasks.register(project=\"flytesnacks\", domain=\"development\", name=\"SagemakerShared\", version=\"1\")\n",
    "sagemaker_lp.register(project=\"flytesnacks\", domain=\"development\", name=\"SagemakerShared\", version=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://flyte.lyft.net/console/projects/flytesnacks/domains/development/executions/fe249ec555287416dadc\n"
     ]
    }
   ],
   "source": [
    "exe=sagemaker_lp.launch(project=\"flytesnacks\", domain=\"development\", inputs={\"loc\":\"SFO\", \"seed\":5, \"num_houses\":1000})\n",
    "print_console_url(exe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "exe.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAIC demo",
   "language": "python",
   "name": "gaic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
