{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ad7306",
   "metadata": {},
   "source": [
    "# Demo of running a Flyte + Feast, feature engineering and training pipeline\n",
    "In this demo we will learn how to interact with Feast through Flyte. The goal will be to train a simple [Gaussian Naive Bayes model using sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) on the [Horse-Colic dataset from UCI](https://archive.ics.uci.edu/ml/datasets/Horse+Colic).\n",
    "The model aims to classify if the lesion of the horse is surgical or not. This is a modified version of the original dataset.\n",
    "\n",
    "**NOTE**\n",
    "We will not really dive into the dataset or the model, as the aim of this tutorial is to show how you can use Feast as the feature store and use Flyte to engineer the features that can be identical across your online predictions as well as offline training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dbb780",
   "metadata": {},
   "source": [
    "## Step 1: Check out the code for the pipeline\n",
    "We have used [flytekit](https://docs.flyte.org/projects/flytekit/en/latest/) flyte's python SDK to express the pipeline in pure python. The actual workflow code is auto-documented and rendered using sphinx [here](https://flyte--424.org.readthedocs.build/projects/cookbook/en/424/auto/case_studies/feature_engineering/feast_integration/index.html) *to be merged soon*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc8e06",
   "metadata": {},
   "source": [
    "## Step 2: Launch an execution\n",
    "We can use the [FlyteConsole](https://github.com/flyteorg/flyteconsole) to launch, monitor and introspect Flyte executions, but in this case we will use [flytekit.remote](https://docs.flyte.org/projects/flytekit/en/latest/design/control_plane.html) to interact with the Flyte backend.\n",
    "\n",
    "### Setup flytekit remote from config\n",
    "To work with flytesandbox, we have created a simple local config that points to FlyteSandbox server and execution environment. We will initialize flytekit remote with this server. We will also pin it to one project and domain.\n",
    "\n",
    "**Note** this also sets up access to S3 or other equivalent datastores needed by FEAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dd18c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from flytekit.remote import FlyteRemote\n",
    "remote = FlyteRemote.from_config(\"flytesnacks\", \"development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd0b83e",
   "metadata": {},
   "source": [
    "### Retrieve the latest registered version of the pipeline\n",
    "FlyteRemote provides convienient methods to retrieve a version of the pipeline from the remote server.\n",
    "\n",
    "**Note** It is possible to get a specific version of workflow and trigger a launch for that, but, we will just get the latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccda3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from feast_integration.feast_workflow import feast_workflow\n",
    "lp = remote.fetch_launch_plan(name=\"feast_integration.feast_workflow.feast_workflow\")\n",
    "lp.id.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8ccc35",
   "metadata": {},
   "source": [
    "### Launch an execution\n",
    "`remote.execute` makes it simple to start an execution for the launchplan. We will not provide any inputs and just use the default inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a461fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "exe = remote.execute(lp, inputs={})\n",
    "print(f\"http://localhost:30081/console/projects/{exe.id.project}/domains/{exe.id.domain}/executions/{exe.id.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb68140",
   "metadata": {},
   "source": [
    "## Step 3: Now wait for the execution to complete\n",
    "It is possible to launch a sync execution and wait for it to complete, but since all the processes are completely detached (you can even close your laptop) and come back to it later, we will show how to sync the execution back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502711ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flytekit.models.core.execution import WorkflowExecutionPhase\n",
    "exe = remote.sync(exe)\n",
    "print(f\"Execution {exe.id.name} is in Phase - {WorkflowExecutionPhase.enum_to_string(exe.closure.phase)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda8da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exe.sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2074cc",
   "metadata": {},
   "source": [
    "## Step 4: Lets sync data from this execution\n",
    "\n",
    "**Side Note**\n",
    "It is possible to fetch an existing execution or simply retrieve a started execution. Also if you launch an execution with the same name, flyte will respect and not restart a new execution!\n",
    "\n",
    "To fetch an execution\n",
    "```python\n",
    "exe = remote.fetch_workflow_execution(name='f9f180a56e67b4c9781e')\n",
    "exe = remote.sync(exe)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e778b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast_dataobjects import FeatureStore\n",
    "fs = exe.raw_outputs.get('o0', FeatureStore)\n",
    "model = exe.outputs['o1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545e00c9",
   "metadata": {},
   "source": [
    "#### Lets inspect the feature store configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dff70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db4c76",
   "metadata": {},
   "source": [
    "#### Also, the model is now available locally as a JobLibSerialized file and can be downloaded and loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3747b3a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ae557b",
   "metadata": {},
   "source": [
    "## Step 5: Cool, Let's predict\n",
    "So we have the model and a feature store!, how can you run predictions. Flytekit will automatically manage the IO for you and you can simply re-use the prediction function from the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c20c5d",
   "metadata": {},
   "source": [
    "### Lets load some features from the online feature store\n",
    "We are re-using the feature definition from the flyte workflow\n",
    "```python\n",
    "inference_point = fs.get_online_features(FEAST_FEATURES, [{\"Hospital Number\": \"533738\"}])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88abda57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast_workflow import predict, FEAST_FEATURES\n",
    "inference_point = fs.get_online_features(FEAST_FEATURES, [{\"Hospital Number\": \"533738\"}])\n",
    "inference_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5745a6",
   "metadata": {},
   "source": [
    "### Now run a prediction\n",
    "Notice how we are passing the serialized model and some loaded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff6c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model_ser=model, features=inference_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1452207b",
   "metadata": {},
   "source": [
    "## Done! \n",
    "We can ofcourse observe the intermediates from the workflow, which we saw in the UI, we can also download any intermediate data.\n",
    "\n",
    "## Future\n",
    "We want to further improve this experience, to allow for the same prediction method to run in your inference server and in a workflow. It is almost there now, but you need to remove the `model de-serialization` as this happens for the current predict method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
