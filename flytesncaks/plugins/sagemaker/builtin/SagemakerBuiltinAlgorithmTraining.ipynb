{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "This is just common setup so that we can load the right flyte configuration and the connect with the right endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flytekit.configuration import set_flyte_config_file, platform\n",
    "set_flyte_config_file(\"/Users/kumare/.ssh/notebook-staging.config\")\n",
    "\n",
    "print(\"Connected to {}\".format(platform.URL.get()))\n",
    "\n",
    "def print_console_url(exc):\n",
    "    print(\"http://{}/console/projects/{}/domains/{}/executions/{}\".format(platform.URL.get(), exc.id.project, exc.id.domain, exc.id.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a XGBoost Training job\n",
    "We will create a job that will train an XGBoost model using the prebuilt algorithms @Sagemaker. Refer to [Sagemaker XGBoost docs here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html). To understand more about XGBoost refer [here](https://xgboost.readthedocs.io/en/latest/). To dive deeper into the Flytekit API refer to [docs](https://lyft.github.io/flyte/flytekit/flytekit.common.tasks.sagemaker.html?highlight=sagemaker#module-flytekit.common.tasks.sagemaker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flytekit.sdk.tasks import inputs\n",
    "from flytekit.sdk.types import Types\n",
    "from flytekit.sdk.workflow import workflow_class, Input, Output\n",
    "from flytekit.common.tasks.sagemaker import training_job_task\n",
    "from flytekit.models.sagemaker import training_job as training_job_models, hpo_job as hpo_job_models\n",
    "\n",
    "# Defining the values of some hyperparameters, which will be used by the TrainingJob \n",
    "# these hyper-parameters are commonly used by the XGboost algorithm. Here we bootstrap them with some default Values\n",
    "# Usually the default values are selected or \"tuned - refer to next section\"\n",
    "xgboost_hyperparameters = {\n",
    "    \"num_round\": \"100\",\n",
    "    \"base_score\": \"0.5\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"csv_weights\": \"0\",\n",
    "    \"dsplit\": \"row\",\n",
    "    \"grow_policy\": \"depthwise\",\n",
    "    \"lambda_bias\": \"0.0\",\n",
    "    \"max_bin\": \"256\",\n",
    "    \"normalize_type\": \"tree\",\n",
    "    \"objective\": \"reg:linear\",\n",
    "    \"one_drop\": \"0\",\n",
    "    \"prob_buffer_row\": \"1.0\",\n",
    "    \"process_type\": \"default\",\n",
    "    \"refresh_leaf\": \"1\",\n",
    "    \"sample_type\": \"uniform\",\n",
    "    \"scale_pos_weight\": \"1.0\",\n",
    "    \"silent\": \"0\",\n",
    "    \"skip_drop\": \"0.0\",\n",
    "    \"tree_method\": \"auto\",\n",
    "    \"tweedie_variance_power\": \"1.5\",\n",
    "    \"updater\": \"grow_colmaker,prune\",\n",
    "}\n",
    "\n",
    "# Here we define the actual algorithm (XGBOOST) and version of the algorithm to use\n",
    "alg_spec = training_job_models.AlgorithmSpecification(\n",
    "    input_mode=training_job_models.InputMode.FILE,\n",
    "    algorithm_name=training_job_models.AlgorithmName.XGBOOST,\n",
    "    algorithm_version=\"0.90\",\n",
    "    input_content_type=training_job_models.InputContentType.TEXT_CSV,\n",
    ")\n",
    "\n",
    "# Finally lets use Flytekit plugin called SdkBuiltinAlgorithmTrainingJobTask, to create a task that wraps the algorithm.\n",
    "# This task does not really have a user-defined function as the actual algorithm is pre-defined in Sagemaker.\n",
    "# But, this task still has the same set of properties like any other FlyteTask\n",
    "# - Caching\n",
    "# - Resource specification\n",
    "# - versioning etc\n",
    "xgboost_train_task = training_job_task.SdkBuiltinAlgorithmTrainingJobTask(\n",
    "    training_job_resource_config=training_job_models.TrainingJobResourceConfig(\n",
    "        instance_type=\"ml.m4.xlarge\",\n",
    "        instance_count=1,\n",
    "        volume_size_in_gb=25,\n",
    "    ),\n",
    "    algorithm_specification=alg_spec,\n",
    "    cache_version='blah9',\n",
    "    cacheable=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the training job\n",
    "You can use [Single task Execution](https://lyft.github.io/flyte/user/features/single_task_execution.html?highlight=single%20task%20execution) to trigger an execution for the defined training task. To trigger an execution, you need to provide\n",
    "1. Project (flyteexamples)\n",
    "1. Domain (development)\n",
    "1. inputs\n",
    "\n",
    "Pre-built algorithms have restricitve set of inputs. They always expect\n",
    "1. Training data set\n",
    "1. Validation data set\n",
    "1. Static set of hyper parameters as a dictionary\n",
    "\n",
    "In this case we have taken the [PIMA Diabetes dataset](https://www.kaggle.com/kumargh/pimaindiansdiabetescsv) and split it and uploaded to an s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs={\n",
    "    \"train\": \"s3://my-bucket/test-datasets/pima/train\",\n",
    "    \"validation\": \"s3://my-bucket/test-datasets/pima/validation\",\n",
    "    \"static_hyperparameters\": xgboost_hyperparameters,\n",
    "}\n",
    "\n",
    "# Invoking the SdkBuiltinAlgorithmTrainingJobTask\n",
    "training_exc = xgboost_train_task.register_and_launch(\"flyteexamples\", \"development\", inputs=training_inputs)\n",
    "print_console_url(training_exc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the hyper-parameters\n",
    "Amazon Sagemaker offers automatic hyper parameter blackbox optimization using [HPO Service](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html). This technique is highly effective to figure out the right set of hyper parameters to use that improve the overall accuracy of the model (or minimize the error)\n",
    "\n",
    "Flyte makes it extremely effective to optimize a model using Amazon Sagemaker HPO. In this example we will look how this can be done for the prebuilt algorithm training done in the previous section\n",
    "\n",
    "### Define an HPO Task, that wraps the training task\n",
    "So to start with hyper parameter optimization, once a training task is created, wrap it in **SdkSimpleHyperparameterTuningJobTask** attribte **training_job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flytekit.common.tasks.sagemaker import hpo_job_task\n",
    "from flytekit.models.sagemaker.hpo_job import HyperparameterTuningJobConfig, \\\n",
    "HyperparameterTuningObjectiveType, HyperparameterTuningStrategy, \\\n",
    "TrainingJobEarlyStoppingType, HyperparameterTuningObjective\n",
    "from flytekit.models.sagemaker.parameter_ranges import HyperparameterScalingType, ParameterRanges, CategoricalParameterRange, ContinuousParameterRange, IntegerParameterRange\n",
    "\n",
    "xgboost_hpo_task = hpo_job_task.SdkSimpleHyperparameterTuningJobTask(\n",
    "    training_job=xgboost_train_task,\n",
    "    max_number_of_training_jobs=10,\n",
    "    max_parallel_training_jobs=5,\n",
    "    cache_version='blah9',\n",
    "    retries=2,\n",
    "    cacheable=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the HPO Job\n",
    "Just like Training Job, HPO job can be launched directly from the notebook\n",
    "The inputs for an HPO job that wraps a training job, are the combination of all inputs for the training job - i.e,\n",
    " 1. \"train\" dataset, \"validation\" dataset and \"static hyper parameters\" for Training job\n",
    " 1. HyperparameterTuningJobConfig, which consists of ParameterRanges, for the parameters that should be tuned,\n",
    " 1. tuning strategy - Bayesian OR Random (or others as described in Sagemaker)\n",
    " 1. Stopping condition and\n",
    " 1. Objective metric name and type (whether to minimize etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When launching the TrainingJob and HPOJob, we need to define the inputs.\n",
    "# Inputs are those directly related to algorithm outputs. We use the inputs\n",
    "# and the version information to decide cache hit/miss\n",
    "\n",
    "hpo_inputs={\n",
    "    \"train\": \"s3://lyft-modelbuilder/test-datasets/pima/train\",\n",
    "    \"validation\": \"s3://lyft-modelbuilder/test-datasets/pima/validation\",\n",
    "    \"static_hyperparameters\": xgboost_hyperparameters,\n",
    "    \"hyperparameter_tuning_job_config\": HyperparameterTuningJobConfig(\n",
    "        \n",
    "        #############################################\n",
    "        # Define the tunable hyperparameters and the \n",
    "        # range/set of possible values of each hp\n",
    "        #############################################\n",
    "        hyperparameter_ranges=ParameterRanges(\n",
    "            parameter_range_map={\n",
    "                \"num_round\": IntegerParameterRange(min_value=3, max_value=10, \n",
    "                                                   scaling_type=HyperparameterScalingType.LINEAR),\n",
    "                \"max_depth\": IntegerParameterRange(min_value=5, max_value=7, \n",
    "                                                   scaling_type=HyperparameterScalingType.LINEAR),\n",
    "                \"gamma\": ContinuousParameterRange(min_value=0.0, max_value=0.3,\n",
    "                                                  scaling_type=HyperparameterScalingType.LINEAR),\n",
    "            }\n",
    "        ),\n",
    "        tuning_strategy=HyperparameterTuningStrategy.BAYESIAN,\n",
    "        tuning_objective=HyperparameterTuningObjective(\n",
    "            objective_type=HyperparameterTuningObjectiveType.MINIMIZE,\n",
    "            metric_name=\"validation:error\",\n",
    "        ),\n",
    "        training_job_early_stopping_type=TrainingJobEarlyStoppingType.AUTO\n",
    "    ).to_flyte_idl(),\n",
    "}\n",
    "\n",
    "## Register and launch the task standalone!\n",
    "hpo_exc = xgboost_hpo_task.register_and_launch(\"flyteexamples\", \"development\", inputs=hpo_inputs)\n",
    "print_console_url(hpo_exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cookbook",
   "language": "python",
   "name": "cookbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
